import numpy as np
import pandas as pd
import gym
import tensorflow as tf
from tensorflow import keras
from collections import deque

# Define the environment
env = gym.make('CartPole-v0')

# Define the DQN architecture
model = keras.Sequential([
    keras.layers.Dense(units=24, activation='relu', input_shape=env.observation_space.shape),
    keras.layers.Dense(units=24, activation='relu'),
    keras.layers.Dense(units=env.action_space.n, activation=None)
])

# Define the hyperparameters
gamma = 0.95
epsilon = 1.0
epsilon_min = 0.01
epsilon_decay = 0.995
learning_rate = 0.001
batch_size = 64
memory = deque(maxlen=100000)

# Compile the model
model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=learning_rate))

# Define the function to perform the action
def act(state):
    if np.random.rand() <= epsilon:
        return env.action_space.sample()
    else:
        return np.argmax(model.predict(state))

# Define the function to update the Q-values
def replay(batch_size):
    minibatch = np.array(memory)[np.random.choice(len(memory), batch_size, replace=False)]
    
    for state, action, reward, next_state, done in minibatch:
        target = reward
        if not done:
            target = reward + gamma * np.amax(model.predict(next_state)[0])
        target_f = model.predict(state)
        target_f[0][action] = target
        
        model.fit(state, target_f, epochs=1, verbose=0)
    
    if epsilon > epsilon_min:
        epsilon *= epsilon_decay

# Train the model
num_episodes = 1000
for episode in range(num_episodes):
    state = env.reset()
    state = np.reshape(state, [1, env.observation_space.shape[0]])
    done = False
    total_reward = 0
    
    for step in range(500):
        action = act(state)
        next_state, reward, done, info = env.step(action)
        total_reward += reward
        next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])
        
        memory.append((state, action, reward, next_state, done))
        state = next_state
        
        if len(memory) > batch_size:
            replay(batch_size)
        
        if done:
            break
    
    if episode % 100 == 0:
        print("Episode: {}/{}, Total Reward: {}, Epsilon: {:.2f}".format(episode, num_episodes, total_reward, epsilon))

# Evaluate the model
total_rewards = []
for episode in range(100):
    state = env.reset()
    state = np.reshape(state, [1, env.obs
